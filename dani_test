import os
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# 1. SETUP: Initialize the LLM
# Replace with your API Key, or use ChatOllama for local models
# os.environ["OPENAI_API_KEY"] = "sk-..." 
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ====================================================
# 2. DEFINE THE EXPERTS (The Destination Chains)
# ====================================================

# Expert A: The Quantitative Analyst (Math/Code)
math_template = """You are a Quantitative Finance Expert. 
You do not talk much; you output Python code or strict mathematical formulas 
to solve the user's problem.
Question: {question}
"""
math_chain = (
    ChatPromptTemplate.from_template(math_template)
    | llm
    | StrOutputParser()
)

# Expert B: The Fundamental Analyst (Sentiment/News)
sentiment_template = """You are a Senior Market Analyst. 
You specialize in reading news, 10-K reports, and gauging market psychology.
Analyze the tone and implications of the user's query.
Question: {question}
"""
sentiment_chain = (
    ChatPromptTemplate.from_template(sentiment_template)
    | llm
    | StrOutputParser()
)

# ====================================================
# 3. DEFINE THE ROUTER (The "Brain")
# ====================================================

# We define a structured output to force the LLM to choose strictly.
class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""
    destination: Literal["math_expert", "sentiment_expert"] = Field(
        ..., 
        description="Choose 'math_expert' for calculations, pricing models, or technical indicators. Choose 'sentiment_expert' for news, opinions, or qualitative analysis."
    )

# Create the router chain that outputs the decision
router_system_prompt = """You are an expert router for a financial system.
Route the input to the best expert based on the user's request."""

router_prompt = ChatPromptTemplate.from_messages([
    ("system", router_system_prompt),
    ("human", "{question}"),
])

# We use 'with_structured_output' to ensure we get a clean Python object back
router_chain = router_prompt | llm.with_structured_output(RouteQuery)

# ====================================================
# 4. THE CONTROL FLOW (The Logic)
# ====================================================

def route_decision(info):
    """
    Receives the output from the router_chain and chooses 
    which expert chain to run next.
    """
    if info["topic"].destination == "math_expert":
        print(f"\n[SYSTEM] ðŸ§® Routing to MATH EXPERT...")
        return math_chain
    else:
        print(f"\n[SYSTEM] ðŸ“° Routing to SENTIMENT EXPERT...")
        return sentiment_chain

# The Full Chain: 
# 1. Calculate the route (and pass the original question through)
# 2. Use RunnableLambda to dynamically pick the next chain
full_chain = (
    {"topic": router_chain, "question": RunnablePassthrough()} 
    | RunnableLambda(route_decision)
)

# ====================================================
# 5. TESTING
# ====================================================

# Example 1: A Math Question
query1 = "Calculate the Sharpe Ratio for a portfolio with 12% return and 15% volatility."
print(f"User: {query1}")
response1 = full_chain.invoke(query1)
print(f"Response:\n{response1}\n")

print("-" * 40)

# Example 2: A Sentiment Question
query2 = "How might the recent geopolitical tension in the Middle East impact oil futures?"
print(f"User: {query2}")
response2 = full_chain.invoke(query2)
print(f"Response:\n{response2}\n")

